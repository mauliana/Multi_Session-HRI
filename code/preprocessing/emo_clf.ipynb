{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emo Model Fine-tuning\n",
    "(helper_models) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv('train_emo_prompt.tsv', sep='\\t')\n",
    "eval_data = pd.read_csv('valid_emo_prompt.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plutchik's emotion:\n",
    "- Anticipating: anticipating, anxious\n",
    "- Joy: joyful, content\n",
    "- Trust: trusting\n",
    "- Fear: afraid, terrified\n",
    "- Surprise: surprised\n",
    "- Sad: sad, lonely, devastated\n",
    "- Disgust: disgusted\n",
    "- Anger: angry, annoyed, furious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plutchik = ['anticipating', 'joy', 'trust', 'fear', 'surprise', 'sad', 'disgust', 'anger']\n",
    "\n",
    "def filter_data(df, label):\n",
    "    # Define a dictionary to map the values\n",
    "    mapping = {\n",
    "        'anxious': 'anticipating',\n",
    "        'joyful': 'joy',\n",
    "        'content': 'joy',\n",
    "        'trusting': 'trust',\n",
    "        'afraid': 'fear',\n",
    "        'terrified': 'fear',\n",
    "        'surprised': 'surprise',\n",
    "        'lonely': 'sad',\n",
    "        'devastated': 'sad',\n",
    "        'disgusted': 'disgust',\n",
    "        'angry': 'anger',\n",
    "        'annoyed': 'anger',\n",
    "        'furious': 'anger'\n",
    "    }\n",
    "    # Replace values in the 'context' column using the dictionary\n",
    "    df['context']  = df['context'].replace(mapping)\n",
    "    filtered_df = df[df['context'].isin(label)]\n",
    "\n",
    "    header = {'context': 'label', 'prompt': 'text'}\n",
    "    filtered_df = filtered_df.rename(columns=header)\n",
    "    return filtered_df\n",
    "\n",
    "train_df = filter_data(train_data, plutchik)\n",
    "eval_df = filter_data(eval_data, plutchik)\n",
    "# train_data['context'] = train_data['context'].replace(mapping)\n",
    "# eval_data['context'] = eval_data['context'].replace(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df.context.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# filtered_df = train_data[train_data['context'].isin(plutchik)]\n",
    "\n",
    "# rename dataframe header\n",
    "# header = {'context': 'label', 'prompt': 'text'}\n",
    "# filtered_df = filtered_df.rename(columns=header)\n",
    "# print(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in plutchik:\n",
    "#     cp = filtered_df[filtered_df['label'] == i]\n",
    "#     print(f\"{i}: {len(cp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create id2label and label2id dictionaries\n",
    "id2label = {i: emotion for i, emotion in enumerate(plutchik)}\n",
    "label2id = {emotion: i for i, emotion in enumerate(plutchik)}\n",
    "\n",
    "train_text = train_df.text.values\n",
    "train_label = [label2id[label] for label in train_df.label.values]\n",
    "\n",
    "eval_text = eval_df.text.values\n",
    "eval_label = [label2id[label] for label in eval_df.label.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodings(texts, tokenizer):\n",
    "    encoded_data = tokenizer(\n",
    "        [text for text in texts],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt',\n",
    "        max_length=64\n",
    "    )\n",
    "    return encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "train_encoding = encodings(train_text, tokenizer)\n",
    "eval_encoding = encodings(eval_text, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "    \n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "train_dataset = CustomDataset(train_encoding, train_label)\n",
    "eval_dataset = CustomDataset(eval_encoding, eval_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(label2id), id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "recall = evaluate.load('recall')\n",
    "precision = evaluate.load(\"precision\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    acc_score = accuracy.compute(predictions=predictions, references=labels)\n",
    "    r_score = recall.compute(predictions=predictions, references=labels, average='macro')\n",
    "    p_score = precision.compute(predictions=predictions, references=labels, average='macro')\n",
    "    f1_score = f1.compute(predictions=predictions, references=labels, average='macro')\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy':round(acc_score['accuracy'], 2),\n",
    "        'precision':round(p_score['precision'], 2),\n",
    "        'recall':round(r_score['recall'], 2),\n",
    "        'f1':round(f1_score['f1'], 2)\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 56/2740 [45:08<36:03:34, 48.37s/it]\n",
      "/home/mauliana/anaconda3/envs/py2/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  4%|▎         | 50/1370 [00:10<04:51,  4.53it/s]\n",
      "  4%|▎         | 51/1370 [00:12<17:24,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0242905616760254, 'eval_accuracy': 0.77, 'eval_precision': 0.77, 'eval_recall': 0.76, 'eval_f1': 0.76, 'eval_runtime': 1.9151, 'eval_samples_per_second': 702.827, 'eval_steps_per_second': 11.488, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 100/1370 [00:22<04:42,  4.50it/s]\n",
      "  7%|▋         | 100/1370 [00:25<04:42,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.002079725265503, 'eval_accuracy': 0.77, 'eval_precision': 0.75, 'eval_recall': 0.77, 'eval_f1': 0.76, 'eval_runtime': 2.2571, 'eval_samples_per_second': 596.352, 'eval_steps_per_second': 9.747, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 150/1370 [00:35<04:38,  4.38it/s]\n",
      " 11%|█         | 150/1370 [00:37<04:38,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0036277770996094, 'eval_accuracy': 0.78, 'eval_precision': 0.77, 'eval_recall': 0.77, 'eval_f1': 0.77, 'eval_runtime': 2.1946, 'eval_samples_per_second': 613.333, 'eval_steps_per_second': 10.025, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 200/1370 [00:48<04:01,  4.84it/s]\n",
      " 15%|█▍        | 201/1370 [00:50<15:43,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.068631887435913, 'eval_accuracy': 0.76, 'eval_precision': 0.76, 'eval_recall': 0.77, 'eval_f1': 0.76, 'eval_runtime': 1.9937, 'eval_samples_per_second': 675.137, 'eval_steps_per_second': 11.035, 'epoch': 1.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 250/1370 [01:00<03:55,  4.75it/s]\n",
      " 18%|█▊        | 250/1370 [01:03<03:55,  4.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.072704315185547, 'eval_accuracy': 0.77, 'eval_precision': 0.78, 'eval_recall': 0.76, 'eval_f1': 0.76, 'eval_runtime': 2.0441, 'eval_samples_per_second': 658.487, 'eval_steps_per_second': 10.763, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 300/1370 [01:13<03:48,  4.68it/s]\n",
      " 22%|██▏       | 300/1370 [01:16<03:48,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.048705816268921, 'eval_accuracy': 0.76, 'eval_precision': 0.76, 'eval_recall': 0.76, 'eval_f1': 0.76, 'eval_runtime': 2.461, 'eval_samples_per_second': 546.937, 'eval_steps_per_second': 8.94, 'epoch': 2.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 350/1370 [01:27<03:36,  4.71it/s]\n",
      " 26%|██▌       | 350/1370 [01:29<03:36,  4.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0000345706939697, 'eval_accuracy': 0.77, 'eval_precision': 0.77, 'eval_recall': 0.76, 'eval_f1': 0.76, 'eval_runtime': 1.9606, 'eval_samples_per_second': 686.528, 'eval_steps_per_second': 11.221, 'epoch': 2.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 400/1370 [01:40<03:19,  4.85it/s]\n",
      " 29%|██▉       | 400/1370 [01:42<03:19,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9458858966827393, 'eval_accuracy': 0.77, 'eval_precision': 0.77, 'eval_recall': 0.76, 'eval_f1': 0.77, 'eval_runtime': 1.8664, 'eval_samples_per_second': 721.176, 'eval_steps_per_second': 11.787, 'epoch': 2.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 450/1370 [01:53<03:33,  4.30it/s]\n",
      " 33%|███▎      | 450/1370 [01:55<03:33,  4.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.942530870437622, 'eval_accuracy': 0.77, 'eval_precision': 0.77, 'eval_recall': 0.76, 'eval_f1': 0.77, 'eval_runtime': 2.144, 'eval_samples_per_second': 627.802, 'eval_steps_per_second': 10.261, 'epoch': 3.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 500/1370 [02:06<03:27,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0052, 'learning_rate': 1.27007299270073e-05, 'epoch': 3.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 36%|███▋      | 500/1370 [02:08<03:27,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9877229928970337, 'eval_accuracy': 0.77, 'eval_precision': 0.76, 'eval_recall': 0.76, 'eval_f1': 0.76, 'eval_runtime': 2.2273, 'eval_samples_per_second': 604.329, 'eval_steps_per_second': 9.878, 'epoch': 3.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 550/1370 [02:38<03:04,  4.45it/s]  \n",
      " 40%|████      | 550/1370 [02:41<03:04,  4.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9841604232788086, 'eval_accuracy': 0.76, 'eval_precision': 0.75, 'eval_recall': 0.76, 'eval_f1': 0.75, 'eval_runtime': 2.3168, 'eval_samples_per_second': 580.982, 'eval_steps_per_second': 9.496, 'epoch': 4.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 600/1370 [02:52<02:41,  4.78it/s]\n",
      " 44%|████▍     | 600/1370 [02:54<03:43,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9617804288864136, 'eval_accuracy': 0.77, 'eval_precision': 0.78, 'eval_recall': 0.76, 'eval_f1': 0.76, 'eval_runtime': 2.0584, 'eval_samples_per_second': 653.918, 'eval_steps_per_second': 10.688, 'epoch': 4.38}\n",
      "{'train_runtime': 174.257, 'train_samples_per_second': 501.73, 'train_steps_per_second': 7.862, 'train_loss': 0.011466334263483683, 'epoch': 4.38}\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback, IntervalStrategy\n",
    "\n",
    "batch_size = 64\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"results-30\",\n",
    "    learning_rate=2e-5,\n",
    "    seed= 42,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=IntervalStrategy.STEPS,\n",
    "    eval_steps = 50,\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub=False,\n",
    "    save_total_limit=2,\n",
    "    logging_dir='logs-30',\n",
    "    metric_for_best_model = 'f1',\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "early_stop = EarlyStoppingCallback(2, 1.0)\n",
    "# early_stop = EarlyStoppingCallback(early_stopping_patience=3)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(output_dir='results-30/best-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: fear\n",
      "Class Probabilities:\n",
      "fear: 1.0000\n",
      "sad: 0.0000\n",
      "surprise: 0.0000\n",
      "disgust: 0.0000\n",
      "anger: 0.0000\n",
      "trust: 0.0000\n",
      "anticipating: 0.0000\n",
      "joy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "# Set the logging level to WARNING or higher to suppress INFO messages\n",
    "logging.getLogger(\"transformers\").setLevel(logging.WARNING)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained('results-30/best-model')\n",
    "\n",
    "new_text = \"some guys shot my neighbour and ran into the woods\"\n",
    "inputs = tokenizer(new_text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "probabilities = torch.softmax(logits, dim=1)\n",
    "predicted_class_index = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "class_labels = plutchik\n",
    "\n",
    "predicted_class = class_labels[predicted_class_index]\n",
    "predicted_probabilities = probabilities[0].tolist()\n",
    "\n",
    "print(\"Predicted Class:\", predicted_class)\n",
    "print(\"Class Probabilities:\")\n",
    "# for label, prob in zip(class_labels, predicted_probabilities):\n",
    "#     print(f\"{label}: {prob:.4f}\")\n",
    "sorted_probabilities = sorted(\n",
    "    zip(class_labels, predicted_probabilities),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "for label, prob in sorted_probabilities:\n",
    "    print(f\"{label}: {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauliana/anaconda3/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:89: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: joy\n",
      "Label Probability:\n",
      "joy: 1.0\n",
      "trust: 0.0\n",
      "surprise: 0.0\n",
      "fear: 0.0\n",
      "disgust: 0.0\n",
      "anger: 0.0\n",
      "anticipating: 0.0\n",
      "sad: 0.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import logging\n",
    "\n",
    "logging.disable(logging.INFO)\n",
    "\n",
    "text = \"I am fine thank you\"\n",
    "classifier = pipeline(\"text-classification\", model=\"results-10/best-model\", return_all_scores=True)\n",
    "predicted_label = classifier(text)\n",
    "\n",
    "# Sort the list of dictionaries based on the 'score' in descending order\n",
    "sorted_data = sorted(predicted_label[0], key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "# Print the sorted data with labels and scores rounded to 2 decimal places\n",
    "print(f\"Predicted label: {sorted_data[0]['label']}\")\n",
    "print('Label Probability:')\n",
    "for item in sorted_data:\n",
    "    label = item['label']\n",
    "    score = round(item['score'], 4)\n",
    "    print(f'{label}: {score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
